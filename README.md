# CyberGuard
Models to classify text for the IndiaAI Cyberguard Hackathon

CyberGuard AI Hackathon
M Sree Abhinav
Jagadesh

Analysis of Problem Statement & Dataset provided

1.	The categories and subcategories in the datasets provided do not match the categories and sub-categories to be classified into for the Hackathon. For this reason, mere cleaning of the dataset provided and then training a model to make predictions will not be able to achieve acceptable accuracy.
2.	There is a lot of class/label imbalance in the dataset provided.

Figure 1: Category distribution in Dataset1[test.csv]
 

Figure 2: Category distribution in Dataset2[train.csv]
 
3.	Other important demographic details of the victim/complainant would also help in classifying the complaint description, for example, crimes related to women and children.
4.	It is possible that the complaint's category will overlap with multiple categories and subcategories.
5.	5. Many text complaints [crimeaditionalinfo] are transliterated from Hindi to English. This poses an additional challenge to any model or approach that considers the context of the words during the learning phase.
6.	There are around 25 subcategories for which there is no corresponding data in either of the datasets.
7.	Three subcategories in the one dataset do not have entries in the other dataset provided.
Date preprocessing
1.	Fill missing sub-categories: In 6591 rows, the subcategories column had no entry. The value from the category column was copied into the subcategories column
2.	Delete duplicates: There are 5998 Duplicate entries in the ‘crimeaditionalinfo’ column, which have been deleted. 
3.	Data cleaning 1: Removed all special characters, multiple spaces, line breaks, tab breaks, repetitive characters from ‘crimeaditionalinfo’ column and made all the characters lowercase.
4.	Data cleaning 2: Removed all rows where the number of words in ‘crimeaditionalinfo’ was less than 5.
5.	Removed rows that have crimeaditionalinfo as null/blank.
6.	StopWords – Removed stopwords for traditional algorithms and not for Transformer algorithms[BERT] to maintain the context
Approach to text classification
	This hackathon presents a classic text classification problem. Two approaches have been identified, though the team is being taken.
1.	Traditional machine learning: Use of word vectorization like word2cev, countvectorize, and tfdif to generate vectors for the data in ‘crimeaditionalinfo’ and then running a traditional ML algorithm like Naïve Bayes, SVM, etc\
2.	Transformer learning: Generation of vectors for the ‘crimeaditionalinfo’ column using tokenizers based on transformer models.
Three strategies have been developed, and accuracies, a confusion matrix, and an F1 score have been generated to establish a benchmark.
	Count Vectorizer & Naïve Bayes
	Vectors for the ' crimeaditionalinfo ' column were generated using the traditional technique of generating word embeddings using Count Vectorizer. Prior to this, since the embeddings were calculated based on the frequency of words, stopwords were removed, and lemmatization was performed.
 
	A NaivesBayes algorithm was then trained based on the Train Dataset, and the model's accuracy scores were calculated on the test split.
 

	An overall accuracy score of 0.45 was achieved. By analysing the classification report[confusion matrix], it was seen that the prediction of minority classes was very poor, and the weighted average accuracy was better than the macro accuracy.


	TF-IDF & Naïve Bayes
	Another traditional approach of using a TF-IDF vectorizer instead was tested.
 
	A NaiveBayes model was trained using the same training dataset, and accuracy was calculated on the Test split.
 
	It was observed that both the accuracy and the macro average dropped. One reason for this could be the imbalance of data, which was directly affecting the vectors generated by the TFIDF vectorization. This is also confirmed by the fact that the F1 score of minority classes was very poor compared to the majority classes, as seen in the classification report.[confusion matrix].
	As a possible remedy, random under and over-sampling of data was tried. As SMOTE was computationally expensive, traditional sampling methods were used. However, overall improvement was needed.
	BERT
	A transformer-based neural network model was developed to improve accuracy and develop a better model that can also understand the context to make predictions.
	Since the task was text classification and not text generation, an encoder-only transformer model like BERT was zeroed in. The pre-trained model would be fine-tuned using the Train dataset provided. The BERT base model has 110 million parameters, and they can all be fine-tuned for the task at hand.
	The preprocessing was done, but the stopwords were intentionally left in so as to maintain context. The tokenization was done using the BERT tokenizer, which is much more advanced than a traditional frequency-based tokenizer. The model was then trained on the Train dataset, and the accuracy was tested on the test dataset.
 
	An accuracy of .55 was achieved. The primary factor impacting the accuracy score is the imbalance in classes and the quality of the training data. Though the macro average improved drastically compared to traditional ML algorithms, it was still low.
	Another strategy used a reduced sample size from the dataset for training. 100 samples from each label class were taken, and for those classes with less than 100 sample sizes, the entire data was taken. However, this did not result in a significant increase in accuracy scores.
Way forward

1.	More data should be captured 
2.	The data should be accurately labelled. The categories should be realigned.
3.	Prediction of the three most probable subcategories rather than a single category could help in better classification
4.	Image classification on incident media files uploaded using CNN, RCNN, YOLO, multimodal LLMs, etc., in conjunction with text data to better classify complaints and take automated actions.
Conclusion
We evaluated the performance of a Large Language Model (LLM) like BERT and a traditional machine learning (ML) algorithm, Naive Bayes, on the given datasets. Our experiments corroborate the belief that LLMs often surpass traditional ML methods in sentiment analysis, spam SMS detection, and multi-label classification tasks. Moreover, the performance of LLMs can be further enhanced through fine-tuning strategies, making the fine-tuned models the top performers, as observed in our study.
Traditional ML and neural network (NN) approaches to text classification typically involve feature extraction, dimensionality reduction, and classifier selection, which can be complex, require domain expertise, and require considerable trial and error. In our study, we applied bag-of-words and TF-IDF methods before training the Naive Bayes classifier.
In contrast, LLMs simplify the text classification process by directly feeding data into the models and obtaining classification results. This straightforward approach eliminates the need for explicit feature extraction or dimensionality reduction, as LLMs inherently encode rich linguistic features through their deep contextual representations.


